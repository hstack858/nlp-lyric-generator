{"cells":[{"cell_type":"markdown","metadata":{"id":"lUz4lDac0FWP"},"source":["# Steps to run this jupyter notebook\n","1. Make sure you have numpy, pandas, tensorflow, keras, and scikitlearn installed in Jupyter\n","2. Make sure the lyrics.csv file is in the same directory as this jupyter notebook\n","3. Make sure the lyric_prompts.csv file is in the same directory as this jupyter notebook\n","4. Run all code cells in order"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":7977,"status":"ok","timestamp":1651715494913,"user":{"displayName":"Henry Stachowiak","userId":"16572899039265083310"},"user_tz":240},"id":"eotDbVFaHOO6"},"outputs":[],"source":["# Imports\n","import io\n","import os\n","import sys\n","import csv\n","import string\n","import numpy as np\n","import pandas as pd\n","import tensorflow\n","from collections import Counter\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n","from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Embedding\n","from keras import Input, Model, backend, utils\n","from keras.layers import *\n","backend.clear_session()"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488},"executionInfo":{"elapsed":891,"status":"error","timestamp":1651715496810,"user":{"displayName":"Henry Stachowiak","userId":"16572899039265083310"},"user_tz":240},"id":"2gmbDjtCIBJe","outputId":"7e75501a-afc8-4b18-fe33-828239b4babc"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n","\n","\n","  exec(code_obj, self.user_global_ns, self.user_ns)\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-31aa56b4e230>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtranslator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lyrics.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"python\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# Drops lyrics with NaN as their value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'id'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/python_parser.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, **kwds)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"readline\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'lyrics.csv'"]}],"source":["# Put song data into dataframe\n","\n","translator = str.maketrans('', '', string.punctuation)\n","\n","df = pd.read_csv(\"lyrics.csv\", sep=\"\\t\", engine=\"python\", encoding=\"utf-8\", error_bad_lines=False)\n","# Drops lyrics with NaN as their value\n","df.index.name = 'id'\n","df = df.dropna()\n","\n","df.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LPNKguWhwcgf"},"outputs":[],"source":["# Text preprocessing to add a \"single_text\" field\n","def generate_single_text_field(data):\n","   text = data['lyrics']\n","   sections = text.split('\\\\n\\\\n')\n","   verses = {'Verse 1': np.nan,'Verse 2':np.nan,'Verse 3':np.nan,'Verse 4':np.nan, 'Chorus':np.nan}\n","   lyrics = str()\n","   single_text = []\n","   response = {}\n","   \n","   for section in sections:\n","       verse_tag = section[section.find('[') + 1:section.find(']')].strip()\n","       if ':' in verse_tag:\n","           verse_tag = verse_tag[:verse_tag.find(':')]\n","       if verse_tag in verses:\n","          single_text += [x.lower().replace('(','').replace(')','').translate(translator) for x in section[section.find(']')+1:].split('\\\\n') if len(x) > 1]\n","       response['single_text'] =  ' \\n'.join(single_text)\n","   return pd.Series(res)\n","\n","\n","df = df.join( df.apply(generate_single_text_field, axis=1), lsuffix=\"_Left\", rsuffix=\"_Right\")\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3fSR06ZYNL8q"},"outputs":[],"source":["# Constants for now/later\n","BATCH_SIZE = 100\n","MIN_SEQ = 5\n","MIN_FREQUENCY = 7\n","text_list = []\n","word_frequencies = Counter()\n","uncommon_words = set()\n","valid_sequences = []\n","end_of_sequence_words = []\n","\n","# Filtering our dataset for common vs. uncommon words\n","\n","def fill_text_list(text):\n","   global text_list\n","   text_list += [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n","\n","df['single_text'].apply( fill_text_list )\n","print('Total words: ', len(text_list))\n","\n","for word in text_list:\n","   word_frequencies[word] = word_frequencies.get(word, 0) + 1\n","\n","uncommon_words = set([key for key in word_frequencies.keys() if word_frequencies[key] < MIN_FREQUENCY])\n","words = sorted(set([key for key in word_frequencies.keys() if word_frequencies[key] >= MIN_FREQUENCY]))\n","\n","num_words = len(words)\n","\n","word_indices = dict((word, index) for index, word in enumerate(words))\n","indices_word = dict((index, word) for index, word in enumerate(words))\n","\n","\n","\n","for i in range(len(text_list) - MIN_SEQ ):\n","   end_slice = i + MIN_SEQ + 1\n","   if len( set(text_list[i:end_slice]).intersection(uncommon_words) ) == 0:\n","       valid_sequences.append(text_list[i: i + MIN_SEQ])\n","       end_of_sequence_words.append(text_list[i + MIN_SEQ])\n","      \n","# Split data into training and testing datasets\n","X_train, X_test, y_train, y_test = train_test_split(valid_sequences, end_of_sequence_words, test_size=0.02, random_state=42)\n","print(X_train[2:5])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JjxVqDPu0A1P"},"outputs":[],"source":["# Data generator for fit and evaluate methods\n","# I had no experience with making generator functions so I used one I found online\n","def generator(sentence_list, next_word_list, batch_size):\n","   index = 0\n","   while True:\n","       x = np.zeros((batch_size, MIN_SEQ), dtype=np.int32)\n","       y = np.zeros((batch_size), dtype=np.int32)\n","       for i in range(batch_size):\n","           for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n","               x[i, t] = word_indices[w]\n","           y[i] = word_indices[next_word_list[index % len(sentence_list)]]\n","           index = index + 1\n","       yield x, y\n","\n","\n","# Functions from keras-team/keras/blob/master/examples/lstm_text_generation.py\n","def sample(preds, temperature=1.0):\n","   # helper function to sample an index from a probability array\n","   preds = np.asarray(preds).astype('float64')\n","   preds = np.log(preds) / temperature\n","   exp_preds = np.exp(preds)\n","   preds = exp_preds / np.sum(exp_preds)\n","   probas = np.random.multinomial(1, preds, 1)\n","   return np.argmax(probas)\n","\n","# This generates new verses of 50 words based on the prompts we made\n","def on_epoch_end(epoch, logs):\n","    examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n","    sentences = []\n","    with open(\"./lyric_prompts.csv\") as file:\n","        reader = csv.reader(file)\n","        for row in reader:\n","            sentences.append(row[0].lower().replace(',',''))\n","    \n","\n","    for sentence in sentences[1:]:\n","        sentence = sentence.split()\n","        examples_file.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n","        examples_file.write(' '.join(sentence))\n","\n","        for i in range(50):\n","            x_pred = np.zeros((1, len(sentence)))\n","            for t, word in enumerate(sentence):\n","                x_pred[0, t] = word_indices[word]\n","\n","            preds = model.predict(x_pred, verbose=0)[0]\n","            next_index = sample(preds, 0.5)\n","            next_word = indices_word[next_index]\n","\n","            sentence = sentence[1:]\n","            sentence.append(next_word)\n","\n","            examples_file.write(\" \"+next_word)\n","        examples_file.write('\\n')\n","    examples_file.write('='*80 + '\\n')\n","    examples_file.flush()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"blVSJew30Fpa"},"outputs":[],"source":["def getModel():\n","   print('Build model...')\n","   model = Sequential()\n","   model.add(Embedding(input_dim=len(words), output_dim=1024))\n","   model.add(Bidirectional(LSTM(128)))\n","   model.add(Dense(len(words)))\n","   model.add(Activation('softmax'))\n","   return model\n","\n","\n","# Commented out code from trying to implement attention mechanism\n","\n","# def getEncoder():\n","#    # int sequences.\n","#    enc_inputs = Input(shape=(20,), name='enc_inputs')\n","\n","   \n","#    # Embedding lookup and GRU\n","#    embedding = Embedding(input_dim=100, output_dim=64)(enc_inputs)\n","#    whole_sequence = GRU(4, return_sequences=True)(embedding)\n","\n","#    # Query-value attention of shape [batch_size, Tq, filters].\n","#    query_value_attention_seq = Attention()([whole_sequence, whole_sequence])\n","\n","#    # build encoder model \n","#    encoder = Model(enc_inputs, query_value_attention_seq, name='encoder')\n","\n","#    return encoder\n","\n","# def getDecoder():\n","#   # int sequences.\n","#   dec_input = Input(shape=(20, 4), name='dec_inputs')\n","\n","#   # LSTM\n","#   whole_sequence = LSTM(4, return_sequences=True)(dec_input)\n","\n","#   # Query-value attention of shape [batch_size, Tq, filters].\n","#   query_value_attention_seq = AdditiveAttention()([whole_sequence, dec_input])\n","\n","#   # Reduce over the sequence axis to produce encodings of shape\n","#   # [batch_size, filters].\n","#   query_value_attention = GlobalAveragePooling1D()(query_value_attention_seq)\n","\n","#   # classification\n","#   dec_output = Dense(1, activation='sigmoid')(query_value_attention)\n","\n","#   # build decoder model\n","#   decoder = Model(dec_input, dec_output, name='decoder')\n","#   return decoder\n","\n","# def getAutoEncoder():\n","#   encoder = getEncoder()\n","#   encoder_init = Input(shape=(20, ))\n","#   encoder_output = encoder(encoder_init)\n","#   print(encoder_output.shape)\n","\n","#   decoder = getDecoder()\n","#   decoder_output = decoder(encoder_output)\n","#   print(decoder_output.shape)\n","\n","#   autoencoder = Model(encoder_init, decoder_output)\n","#   return autoencoder\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J_Bodze40OYp"},"outputs":[],"source":["# model = getAutoEncoder()\n","model = getModel()\n","\n","model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n","\n","\n","# Code from online to save metrics to a file\n","file_path = \"./checkpoints/LSTM_LYRICS-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-loss{loss:.4f}-acc{accuracy:.4f}-val_loss{val_loss:.4f}-val_acc{val_accuracy:.4f}\" % \\\n","            (len(words), MIN_SEQ, MIN_FREQUENCY)\n","\n","\n","checkpoint = ModelCheckpoint(file_path, monitor='val_accuracy', save_best_only=True)\n","\n","print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n","\n","early_stopping = EarlyStopping(monitor='val_accuracy', patience=20)\n","\n","callbacks_list = [checkpoint, print_callback, early_stopping]\n","\n","\n","examples_file = open('examples.txt', \"w\")\n","\n","model.fit(generator(X_train, y_train, BATCH_SIZE),\n","                   steps_per_epoch=100, #int(len(valid_seqs)/BATCH_SIZE) + 1,\n","                   epochs=1,\n","                   callbacks=callbacks_list,\n","                   validation_data=generator(X_test, y_train, BATCH_SIZE),\n","                   validation_steps=int(len(y_train)/BATCH_SIZE) + 1)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"song_lyric_generator (2).ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":0}